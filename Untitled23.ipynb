{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwnVdgYdJXAO"
      },
      "source": [
        "#Ensemble Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJoW7Uf7JhVk"
      },
      "source": [
        "## Theoretical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ8ErXlLJk14"
      },
      "source": [
        "1. Can we use Bagging for regression problems?\n",
        "\n",
        "Answer:\n",
        "\n",
        "**Bagging for Regression Problems**\n",
        "\n",
        "Bagging (Bootstrap Aggregating) is an ensemble technique that can be applied to both **classification and regression** tasks. The main idea of bagging is to reduce variance and improve the stability of a model by combining the predictions of multiple base learners trained on different bootstrap samples of the training data.\n",
        "\n",
        "In the case of **regression problems**:\n",
        "\n",
        "1. Multiple bootstrap samples are drawn from the original dataset.\n",
        "2. A base regressor (commonly a Decision Tree) is trained on each bootstrap sample.\n",
        "3. For prediction, instead of majority voting (used in classification), the outputs of all regressors are **averaged**.\n",
        "\n",
        "This averaging process helps to smooth out the noise present in individual models and produces a more robust prediction. Bagging is particularly effective with high-variance models like decision trees, which tend to overfit the training data when used alone.\n",
        "\n",
        "**Advantages of Bagging in Regression:**\n",
        "\n",
        "* Reduces variance of predictions.\n",
        "* Improves generalization performance.\n",
        "* Makes predictions more stable and less sensitive to fluctuations in the training set.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "* Bagging Regressor in scikit-learn (`sklearn.ensemble.BaggingRegressor`).\n",
        "* Random Forest Regressor, which is an extension of bagging with additional random feature selection.\n",
        "\n",
        "**Conclusion:**\n",
        "Yes, bagging can be effectively used for regression problems. It works by training multiple regressors on bootstrapped datasets and averaging their outputs, leading to improved accuracy and reduced overfitting compared to a single regressor.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDlURPHtPO6I"
      },
      "source": [
        "**2. What is Multiple Model Training?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Multiple Model Training (Ensemble Learning)**\n",
        "\n",
        "Multiple model training involves training **several models** and then combining their outputs to make the final prediction. This approach is also known as **ensemble learning**. By aggregating predictions, it reduces variance, bias, and improves accuracy compared to a single model.\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "1. Several models are trained on data (same or resampled).\n",
        "2. Final prediction is obtained by **averaging** (regression) or **voting** (classification).\n",
        "3. Provides better generalization and stability.\n",
        "4. More computationally expensive and harder to interpret.\n",
        "5. Works well with high-variance models like decision trees.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "* Bagging\n",
        "* Boosting\n",
        "* Random Forest\n",
        "* Stacking\n",
        "\n",
        "**Conclusion:**\n",
        "Multiple model training combines the strengths of different models, making predictions more accurate and robust, though with higher complexity.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13pFPTWIPVzE"
      },
      "source": [
        "**3. Explain the concept of Feature Randomness in Random Forest**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Feature Randomness in Random Forest**\n",
        "\n",
        "Random Forest is an ensemble learning method based on **bagging** with additional randomness introduced to reduce correlation between trees. One of the key techniques used in Random Forest is **feature randomness** (also called *random feature selection*).\n",
        "\n",
        "Instead of considering **all features** while splitting a node (as done in a normal decision tree), Random Forest randomly selects a **subset of features** at each split. The best split is then chosen only from this random subset.\n",
        "\n",
        "**Working of Feature Randomness:**\n",
        "\n",
        "1. For each split in a decision tree, Random Forest selects a random subset of features.\n",
        "2. Among these features, the best split is chosen based on a criterion (e.g., Gini index, Information Gain, MSE).\n",
        "3. This process ensures that not all trees focus on the same dominant features, leading to more diverse trees.\n",
        "\n",
        "**Advantages of Feature Randomness:**\n",
        "\n",
        "* Reduces correlation between trees in the forest.\n",
        "* Improves model diversity and generalization.\n",
        "* Prevents over-reliance on strong predictors.\n",
        "* Helps in handling high-dimensional data.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* In classification, typically √(number of features) are selected at each split.\n",
        "* In regression, typically (number of features)/3 are selected at each split.\n",
        "\n",
        "**Conclusion:**\n",
        "Feature randomness ensures that Random Forest trees are diverse and less correlated. By selecting a random subset of features at each split, the model achieves higher accuracy, robustness, and better generalization compared to standard decision trees.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUjrCw9vPf52"
      },
      "source": [
        "**4. What is OOB (Out-of-Bag) Score?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**OOB (Out-of-Bag) Score**\n",
        "\n",
        "In Random Forest, each tree is trained on a **bootstrap sample** of the dataset. This means some data points are selected (with replacement) to train the tree, while about **one-third of the data is left out** from that sample. These left-out data points are called **Out-of-Bag (OOB) samples**.\n",
        "\n",
        "The **OOB Score** is the performance measure of the Random Forest calculated using these OOB samples. For each data point, the prediction is made using only the trees that did not include that data point in their training. The accuracy (for classification) or error (for regression) of these predictions is called the OOB Score.\n",
        "\n",
        "**Working of OOB Score:**\n",
        "\n",
        "1. About 63% of data is used to train each tree (bootstrap sample).\n",
        "2. The remaining ~37% is OOB data, not seen by that tree.\n",
        "3. Each OOB sample is predicted using the trees where it was not included.\n",
        "4. The aggregated results form the OOB Score, which is an unbiased estimate of test accuracy.\n",
        "\n",
        "**Advantages of OOB Score:**\n",
        "\n",
        "* Provides an **internal cross-validation** within Random Forest.\n",
        "* Eliminates the need for a separate validation dataset.\n",
        "* Saves computation and data resources.\n",
        "* Offers an unbiased estimate of generalization error.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* In scikit-learn, setting `oob_score=True` in `RandomForestClassifier` or `RandomForestRegressor` automatically computes the OOB Score.\n",
        "\n",
        "**Conclusion:**\n",
        "The OOB Score is an internal validation technique in Random Forest that evaluates model performance using out-of-bag samples. It provides a reliable, unbiased estimate of accuracy without needing a separate test set.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUm9t5qpP4om"
      },
      "source": [
        "**5. How can you measure the importance of features in a Random Forest model?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Feature Importance in Random Forest**\n",
        "\n",
        "Random Forest can naturally measure the **importance of features** used in making predictions. Feature importance indicates how much each feature contributes to reducing error in the model. Random Forest computes this by evaluating how much each feature improves the **splitting criterion** (e.g., Gini impurity for classification, MSE for regression) across all trees in the forest.\n",
        "\n",
        "**Methods to Measure Feature Importance:**\n",
        "\n",
        "1. **Mean Decrease in Impurity (MDI):**\n",
        "\n",
        "   * For each feature, calculate how much the splitting criterion (like Gini impurity or MSE) decreases when the feature is used in a node.\n",
        "   * Average this decrease across all trees.\n",
        "   * Features causing larger decreases are more important.\n",
        "\n",
        "2. **Mean Decrease in Accuracy (Permutation Importance):**\n",
        "\n",
        "   * Randomly shuffle the values of a feature in the dataset.\n",
        "   * Measure the decrease in model accuracy or increase in error after shuffling.\n",
        "   * Features that cause a larger drop in performance are more important.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Identifies the most influential features for predictions.\n",
        "* Helps in feature selection and dimensionality reduction.\n",
        "* Improves model interpretability.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* In scikit-learn, `feature_importances_` attribute of `RandomForestClassifier` or `RandomForestRegressor` gives the importance score for each feature.\n",
        "* Permutation importance can be calculated using `permutation_importance` function in scikit-learn.\n",
        "\n",
        "**Conclusion:**\n",
        "Feature importance in Random Forest helps identify which features contribute most to prediction accuracy. It can be measured using either the **decrease in impurity** across trees or the **drop in accuracy** after feature permutation.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqb6qz5HP8LJ"
      },
      "source": [
        "**6. Explain the working principle of a Bagging Classifier**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Bagging Classifier**\n",
        "\n",
        "Bagging (Bootstrap Aggregating) is an ensemble learning technique used to improve the accuracy and stability of machine learning models, particularly **high-variance models** like decision trees. A Bagging Classifier combines the predictions of multiple base classifiers trained on different subsets of the training data.\n",
        "\n",
        "**Working Principle:**\n",
        "\n",
        "1. **Bootstrap Sampling:**\n",
        "\n",
        "   * Generate multiple **bootstrap samples** from the original training dataset.\n",
        "   * Each sample is created by randomly selecting data points **with replacement**, so some points may appear multiple times while others may be left out.\n",
        "\n",
        "2. **Training Base Classifiers:**\n",
        "\n",
        "   * A separate base classifier (e.g., Decision Tree) is trained on each bootstrap sample.\n",
        "   * Each classifier learns independently from its own subset of data.\n",
        "\n",
        "3. **Making Predictions:**\n",
        "\n",
        "   * For **classification**, the final prediction is made by **majority voting** among all base classifiers.\n",
        "   * For **regression**, predictions are **averaged** instead of voting.\n",
        "\n",
        "4. **Reducing Variance:**\n",
        "\n",
        "   * Aggregating multiple classifiers reduces the overall variance of the model.\n",
        "   * Each classifier may overfit its bootstrap sample, but averaging their predictions stabilizes the results.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Reduces overfitting and variance.\n",
        "* Improves prediction accuracy.\n",
        "* Simple to implement and parallelize.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "* `BaggingClassifier` in scikit-learn.\n",
        "* Random Forest Classifier (a special case of bagging with feature randomness).\n",
        "\n",
        "**Conclusion:**\n",
        "A Bagging Classifier works by training multiple models on bootstrapped datasets and combining their predictions using majority voting. This ensemble approach produces more stable and accurate predictions compared to a single classifier.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AjPl2DqQOo7"
      },
      "source": [
        "**7. How do you evaluate a Bagging Classifier’s performance?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Evaluating a Bagging Classifier**\n",
        "\n",
        "A Bagging Classifier is an ensemble of multiple base classifiers. Its performance can be evaluated using standard classification metrics and additional ensemble-specific techniques.\n",
        "\n",
        "**Methods to Evaluate Performance:**\n",
        "\n",
        "1. **Train-Test Split / Cross-Validation:**\n",
        "\n",
        "   * Split the dataset into training and testing sets or use k-fold cross-validation.\n",
        "   * Train the Bagging Classifier on the training set and evaluate predictions on the test set.\n",
        "\n",
        "2. **Accuracy:**\n",
        "\n",
        "   * Measure the proportion of correctly predicted samples:\n",
        "     [\n",
        "     \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Predictions}}\n",
        "     ]\n",
        "\n",
        "3. **Precision, Recall, and F1-Score:**\n",
        "\n",
        "   * For imbalanced datasets, accuracy may be misleading.\n",
        "   * Precision measures the proportion of true positive predictions among all positive predictions.\n",
        "   * Recall measures the proportion of true positive predictions among all actual positives.\n",
        "   * F1-Score is the harmonic mean of precision and recall.\n",
        "\n",
        "4. **Confusion Matrix:**\n",
        "\n",
        "   * Summarizes predictions versus actual labels for each class.\n",
        "   * Helps identify which classes are misclassified.\n",
        "\n",
        "5. **Out-of-Bag (OOB) Score:**\n",
        "\n",
        "   * Since Bagging uses bootstrap sampling, about 1/3 of the training data is left out for each base classifier.\n",
        "   * OOB score is computed by predicting these left-out samples using the corresponding classifiers.\n",
        "   * Provides an unbiased estimate of generalization accuracy without a separate validation set.\n",
        "\n",
        "6. **ROC Curve and AUC (for binary classification):**\n",
        "\n",
        "   * Plot True Positive Rate vs. False Positive Rate at different thresholds.\n",
        "   * AUC measures the model’s ability to distinguish between classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Tfyt1tf2QSZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "900ae2ad-f1a8-48a9-a3b9-ad0f72184bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n",
            "OOB Score: 0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = BaggingClassifier(n_estimators=50, oob_score=True, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"OOB Score:\", model.oob_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR0fC57rQeKk"
      },
      "source": [
        "**Conclusion:**\n",
        "The performance of a Bagging Classifier is evaluated using **accuracy, precision, recall, F1-score, confusion matrix, ROC-AUC**, and optionally the **OOB score**. These metrics provide a comprehensive assessment of prediction accuracy and generalization.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq_A2BOSQihS"
      },
      "source": [
        "**8. How does a Bagging Regressor work?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Bagging Regressor**\n",
        "\n",
        "A Bagging Regressor is an ensemble learning technique used to improve the stability and accuracy of regression models. It is based on **bagging (Bootstrap Aggregating)** and works by combining predictions from multiple base regressors trained on different subsets of the data.\n",
        "\n",
        "**Working Principle:**\n",
        "\n",
        "1. **Bootstrap Sampling:**\n",
        "\n",
        "   * Multiple bootstrap samples are generated from the original training dataset by **random sampling with replacement**.\n",
        "   * Each sample may contain repeated data points and leave out others.\n",
        "\n",
        "2. **Training Base Regressors:**\n",
        "\n",
        "   * A separate base regressor (e.g., Decision Tree Regressor) is trained on each bootstrap sample independently.\n",
        "\n",
        "3. **Making Predictions:**\n",
        "\n",
        "   * For a new input, each trained regressor makes a prediction.\n",
        "   * The final output of the Bagging Regressor is obtained by **averaging the predictions** of all base regressors.\n",
        "\n",
        "4. **Reducing Variance:**\n",
        "\n",
        "   * Individual regressors may overfit their respective samples, but averaging their predictions reduces overall variance and produces more stable results.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Reduces overfitting and variance.\n",
        "* Improves prediction accuracy and robustness.\n",
        "* Can handle high-variance base models effectively.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "* `BaggingRegressor` in scikit-learn.\n",
        "* Random Forest Regressor is a special case of bagging with added feature randomness.\n",
        "\n",
        "**Conclusion:**\n",
        "A Bagging Regressor works by training multiple regressors on different bootstrap samples and averaging their predictions. This ensemble approach produces more reliable, accurate, and stable regression results compared to a single regressor.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28TbDLq6T_QX"
      },
      "source": [
        "**9. What is the main advantage of ensemble techniques?**  \n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "**Advantage of Ensemble Techniques**  \n",
        "\n",
        "Ensemble techniques combine predictions from **multiple models** to produce a single, improved prediction. The main advantage of ensemble methods is that they **enhance the overall performance and robustness** of a model compared to individual models.  \n",
        "\n",
        "**Key Points:**  \n",
        "\n",
        "1. **Improved Accuracy:**  \n",
        "   - Combining multiple models reduces errors and increases prediction accuracy.  \n",
        "\n",
        "2. **Reduced Overfitting:**  \n",
        "   - Ensemble methods like Bagging and Random Forest reduce variance and prevent overfitting of high-variance models.  \n",
        "\n",
        "3. **Better Generalization:**  \n",
        "   - Ensembles generalize better on unseen data by leveraging the strengths of different models.  \n",
        "\n",
        "4. **Robustness to Noise:**  \n",
        "   - Aggregating multiple models reduces the impact of noisy data or outliers.  \n",
        "\n",
        "**Examples of Ensemble Techniques:**  \n",
        "* Bagging (Bootstrap Aggregating)  \n",
        "* Boosting (e.g., AdaBoost, Gradient Boosting)  \n",
        "* Random Forest  \n",
        "* Stacking  \n",
        "\n",
        "**Conclusion:**  \n",
        "The main advantage of ensemble techniques is that they **produce more accurate, stable, and generalizable predictions** than individual models by combining the strengths of multiple learners.  \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2fRqojgUN7I"
      },
      "source": [
        "**10. What is the main challenge of ensemble methods?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Challenges of Ensemble Methods**\n",
        "\n",
        "While ensemble methods improve accuracy and robustness, they also come with certain challenges. The **main challenge** is **increased complexity and computational cost**.\n",
        "\n",
        "**Key Points:**\n",
        "\n",
        "1. **High Computational Cost:**\n",
        "\n",
        "   * Training multiple models simultaneously requires more **memory, processing power, and time** compared to a single model.\n",
        "\n",
        "2. **Reduced Interpretability:**\n",
        "\n",
        "   * Ensemble models like Random Forest or Gradient Boosting are **harder to interpret** because predictions are based on many combined models.\n",
        "\n",
        "3. **Implementation Complexity:**\n",
        "\n",
        "   * Designing and tuning ensemble methods (choosing base models, number of estimators, hyperparameters) is more **complex** than using a single model.\n",
        "\n",
        "4. **Diminishing Returns:**\n",
        "\n",
        "   * Adding more models does not always significantly improve performance after a certain point.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "* Random Forest may require hundreds of trees, increasing computation.\n",
        "* Gradient Boosting can be slow to train on large datasets.\n",
        "\n",
        "**Conclusion:**\n",
        "The main challenge of ensemble methods is that they **increase computational complexity and reduce interpretability**, making them more resource-intensive and harder to understand compared to single models.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_o1UMdaUSbf"
      },
      "source": [
        "**11. Explain the key idea behind ensemble techniques**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Key Idea of Ensemble Techniques**\n",
        "\n",
        "Ensemble techniques are based on the principle that **combining multiple models** can produce better predictions than a single model. The key idea is to **leverage the strengths of different models** and reduce their weaknesses, leading to improved accuracy, stability, and generalization.\n",
        "\n",
        "**Working Principle:**\n",
        "\n",
        "1. **Diversity Among Models:**\n",
        "\n",
        "   * Multiple models (base learners) are trained using different algorithms, subsets of data, or random variations to ensure diversity.\n",
        "\n",
        "2. **Aggregation of Predictions:**\n",
        "\n",
        "   * The predictions of all models are combined to make the final prediction:\n",
        "\n",
        "     * **Classification:** Majority voting or weighted voting.\n",
        "     * **Regression:** Averaging the predictions.\n",
        "\n",
        "3. **Error Reduction:**\n",
        "\n",
        "   * By combining multiple models, ensemble techniques reduce **bias**, **variance**, or both, depending on the method used (e.g., Bagging reduces variance, Boosting reduces bias).\n",
        "\n",
        "**Advantages of the Key Idea:**\n",
        "\n",
        "* Improves prediction accuracy.\n",
        "* Reduces overfitting and variance.\n",
        "* Produces more robust and generalizable models.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "* Bagging (Random Forest)\n",
        "* Boosting (AdaBoost, Gradient Boosting)\n",
        "* Stacking\n",
        "\n",
        "**Conclusion:**\n",
        "The key idea behind ensemble techniques is that **“multiple weak or base models combined intelligently can outperform a single strong model”**, resulting in more accurate, stable, and reliable predictions.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkGaCpV7Uah5"
      },
      "source": [
        "**12. What is a Random Forest Classifier?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Random Forest Classifier**\n",
        "\n",
        "A Random Forest Classifier is an **ensemble learning method** used for classification tasks. It combines the predictions of multiple **decision trees** to produce a single, more accurate and stable prediction. Random Forest is based on **bagging (bootstrap aggregating)** with an added feature of **random feature selection** to increase diversity among trees.\n",
        "\n",
        "**Working Principle:**\n",
        "\n",
        "1. **Bootstrap Sampling:**\n",
        "\n",
        "   * Multiple decision trees are trained on different **random subsets** of the training data generated by sampling **with replacement**.\n",
        "\n",
        "2. **Random Feature Selection:**\n",
        "\n",
        "   * At each split in a tree, only a **random subset of features** is considered for splitting, ensuring that trees are diverse and less correlated.\n",
        "\n",
        "3. **Prediction Aggregation:**\n",
        "\n",
        "   * For **classification**, each tree predicts a class, and the final output is decided by **majority voting** among all trees.\n",
        "   * For **regression**, predictions are **averaged** across trees.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Reduces overfitting compared to a single decision tree.\n",
        "* Handles large datasets and high-dimensional features well.\n",
        "* Provides feature importance scores.\n",
        "* Robust to noise and outliers.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "* `RandomForestClassifier` in scikit-learn.\n",
        "* Used in applications like spam detection, medical diagnosis, and image classification.\n",
        "\n",
        "**Conclusion:**\n",
        "A Random Forest Classifier works by combining multiple decision trees trained on random subsets of data and features, using majority voting to make predictions. This ensemble approach improves accuracy, robustness, and generalization compared to individual decision trees.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW8ajqhrUovQ"
      },
      "source": [
        "**13. What are the main types of ensemble techniques?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Main Types of Ensemble Techniques**\n",
        "\n",
        "Ensemble techniques combine multiple models to improve prediction accuracy, stability, and generalization. They are broadly classified into three main types based on how the base models are built and combined:\n",
        "\n",
        "1. **Bagging (Bootstrap Aggregating):**\n",
        "\n",
        "   * Multiple base models are trained independently on **different bootstrap samples** of the dataset.\n",
        "   * Predictions are combined using **majority voting** (classification) or **averaging** (regression).\n",
        "   * Reduces **variance** and overfitting.\n",
        "   * **Example:** Random Forest.\n",
        "\n",
        "2. **Boosting:**\n",
        "\n",
        "   * Models are trained **sequentially**, with each new model focusing on the **errors of previous models**.\n",
        "   * Reduces **bias** and improves prediction accuracy.\n",
        "   * **Example:** AdaBoost, Gradient Boosting, XGBoost.\n",
        "\n",
        "3. **Stacking (Stacked Generalization):**\n",
        "\n",
        "   * Combines predictions from **multiple different types of models** using a **meta-model**.\n",
        "   * Base models are trained first, and a higher-level model learns to combine their outputs optimally.\n",
        "   * Can reduce both bias and variance if properly designed.\n",
        "   * **Example:** Using Logistic Regression as a meta-model over Decision Trees and SVMs.\n",
        "\n",
        "**Conclusion:**\n",
        "The main types of ensemble techniques are **Bagging, Boosting, and Stacking**, each with a unique approach to combining multiple models to improve accuracy, stability, and generalization.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9rVRo-XUp6o"
      },
      "source": [
        "**14. What is Ensemble Learning in Machine Learning?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Ensemble Learning**\n",
        "\n",
        "Ensemble learning is a technique in machine learning where **multiple models (called base learners) are combined** to solve a problem and improve performance. The main idea is that a group of models working together can produce **more accurate, stable, and robust predictions** than any single model.\n",
        "\n",
        "**Working Principle:**\n",
        "\n",
        "1. **Diversity Among Models:**\n",
        "\n",
        "   * Different models are trained using different algorithms, data subsets, or features to ensure diversity.\n",
        "\n",
        "2. **Combining Predictions:**\n",
        "\n",
        "   * The outputs of individual models are combined to produce a final prediction:\n",
        "\n",
        "     * **Classification:** Majority voting or weighted voting.\n",
        "     * **Regression:** Averaging predictions.\n",
        "\n",
        "3. **Error Reduction:**\n",
        "\n",
        "   * Combining models reduces **bias**, **variance**, or both, leading to better generalization on unseen data.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Improves prediction accuracy.\n",
        "* Reduces overfitting and variance.\n",
        "* Produces more robust and reliable models.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "* Bagging (Random Forest)\n",
        "* Boosting (AdaBoost, Gradient Boosting)\n",
        "* Stacking\n",
        "\n",
        "**Conclusion:**\n",
        "Ensemble learning is a machine learning approach that **combines multiple models** to achieve better predictive performance than individual models, leveraging their strengths and compensating for weaknesses.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ljtc6y2Uzfz"
      },
      "source": [
        "**15. When should we avoid using ensemble methods?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Avoiding Ensemble Methods**\n",
        "\n",
        "While ensemble methods improve accuracy and robustness, they are not always the best choice. They should be avoided in certain situations due to complexity, resource requirements, or interpretability concerns.\n",
        "\n",
        "**Situations to Avoid Ensemble Methods:**\n",
        "\n",
        "1. **Simple Problems:**\n",
        "\n",
        "   * If the dataset is small or the problem is simple, a **single model** may perform sufficiently well.\n",
        "   * Using an ensemble may **unnecessarily increase complexity** without significant gains.\n",
        "\n",
        "2. **Need for Interpretability:**\n",
        "\n",
        "   * Ensemble methods like Random Forest, Gradient Boosting, or Stacking are **harder to interpret**.\n",
        "   * If model interpretability is crucial (e.g., in medical or financial decisions), single models like Logistic Regression or Decision Trees may be preferred.\n",
        "\n",
        "3. **Limited Computational Resources:**\n",
        "\n",
        "   * Ensembles require **more memory and processing power** due to multiple models being trained.\n",
        "   * On devices with limited resources or for real-time applications, they may be inefficient.\n",
        "\n",
        "4. **Diminishing Returns:**\n",
        "\n",
        "   * After a certain number of base models, adding more may not significantly improve performance.\n",
        "   * Training large ensembles may waste time and resources with minimal benefit.\n",
        "\n",
        "**Conclusion:**\n",
        "Ensemble methods should be avoided when the problem is simple, interpretability is important, computational resources are limited, or when additional models do not substantially improve performance. In such cases, a **single well-tuned model** may be more practical.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdNxaFR2U9ET"
      },
      "source": [
        "**16. How does Bagging help in reducing overfitting?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Bagging and Overfitting**\n",
        "\n",
        "Bagging (Bootstrap Aggregating) is an ensemble technique that helps **reduce overfitting** by combining predictions from multiple models trained on different subsets of the data. Overfitting occurs when a model learns noise or fluctuations in the training data, leading to poor generalization on unseen data.\n",
        "\n",
        "**Working Principle:**\n",
        "\n",
        "1. **Bootstrap Sampling:**\n",
        "\n",
        "   * Multiple **bootstrap samples** are generated from the training dataset by sampling **with replacement**.\n",
        "   * Each sample is slightly different, so individual models see varied data.\n",
        "\n",
        "2. **Training Base Models Independently:**\n",
        "\n",
        "   * A separate model (commonly a decision tree) is trained on each bootstrap sample.\n",
        "   * Trees trained on different subsets capture different patterns and errors.\n",
        "\n",
        "3. **Aggregating Predictions:**\n",
        "\n",
        "   * Predictions from all models are combined:\n",
        "\n",
        "     * **Classification:** Majority voting\n",
        "     * **Regression:** Averaging\n",
        "   * Aggregation **smooths out errors** of individual models and prevents any single model from dominating predictions.\n",
        "\n",
        "**Effect on Overfitting:**\n",
        "\n",
        "* High-variance models like decision trees tend to overfit individual samples.\n",
        "* Bagging reduces the overall variance by **averaging multiple models**, making predictions more stable and generalizable.\n",
        "* Even though individual trees may overfit, the ensemble as a whole is less likely to overfit.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* Random Forest is a bagging-based method where many decision trees are trained on bootstrapped samples with random feature selection, significantly reducing overfitting compared to a single decision tree.\n",
        "\n",
        "**Conclusion:**\n",
        "Bagging reduces overfitting by training multiple models on varied data samples and aggregating their predictions, thereby lowering variance and improving generalization on unseen data.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWED6saKVIpN"
      },
      "source": [
        "**17. Why is Random Forest better than a single Decision Tree?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Random Forest vs. Single Decision Tree**\n",
        "\n",
        "Random Forest is an ensemble learning method based on **bagging**, which combines multiple decision trees to produce more accurate and stable predictions. It is generally better than a single decision tree for several reasons:\n",
        "\n",
        "**Key Advantages:**\n",
        "\n",
        "1. **Reduces Overfitting:**\n",
        "\n",
        "   * Single decision trees often **overfit** the training data, especially when they are deep.\n",
        "   * Random Forest reduces overfitting by **aggregating predictions** from multiple trees trained on different bootstrap samples.\n",
        "\n",
        "2. **Higher Accuracy:**\n",
        "\n",
        "   * Combining multiple trees improves overall prediction accuracy.\n",
        "   * Errors from individual trees tend to cancel out when aggregated.\n",
        "\n",
        "3. **Handles Variance Better:**\n",
        "\n",
        "   * Decision trees are high-variance models.\n",
        "   * Random Forest reduces variance by averaging predictions across many trees, leading to more stable and reliable results.\n",
        "\n",
        "4. **Feature Randomness Increases Diversity:**\n",
        "\n",
        "   * Random Forest considers a **random subset of features** at each split, making trees less correlated.\n",
        "   * This improves generalization and prevents any single dominant feature from controlling predictions.\n",
        "\n",
        "5. **Robustness to Noise and Outliers:**\n",
        "\n",
        "   * Aggregation across multiple trees reduces the impact of noisy data or outliers.\n",
        "\n",
        "6. **Feature Importance Measurement:**\n",
        "\n",
        "   * Random Forest provides estimates of **feature importance**, which is not available from a single tree reliably.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* In scikit-learn, `DecisionTreeClassifier` may overfit on complex datasets, whereas `RandomForestClassifier` gives higher accuracy and better generalization.\n",
        "\n",
        "**Conclusion:**\n",
        "Random Forest is better than a single decision tree because it **reduces overfitting, lowers variance, increases accuracy, and is more robust**, making it a more reliable and generalizable model for both classification and regression tasks.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDmwYg5XVVgF"
      },
      "source": [
        "**18. What is the role of bootstrap sampling in Bagging?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Bootstrap Sampling in Bagging**\n",
        "\n",
        "Bootstrap sampling is a key step in Bagging (Bootstrap Aggregating) that helps create **diversity among base models** and improves the ensemble’s performance. It involves generating multiple **random subsets of the training data** by sampling **with replacement**.\n",
        "\n",
        "**Working Principle:**\n",
        "\n",
        "1. **Creating Bootstrap Samples:**\n",
        "\n",
        "   * From the original training dataset of size (N), multiple subsets of size (N) are created by random sampling **with replacement**.\n",
        "   * Some data points may appear multiple times in a sample, while others may be left out.\n",
        "\n",
        "2. **Training Base Models:**\n",
        "\n",
        "   * Each bootstrap sample is used to train a separate base model (e.g., a decision tree).\n",
        "   * Since the samples differ, each model learns slightly different patterns and errors.\n",
        "\n",
        "3. **Aggregating Predictions:**\n",
        "\n",
        "   * Predictions from all models are combined:\n",
        "\n",
        "     * **Classification:** Majority voting\n",
        "     * **Regression:** Averaging\n",
        "\n",
        "**Role of Bootstrap Sampling:**\n",
        "\n",
        "* Introduces **randomness and diversity** among base models, making the ensemble stronger.\n",
        "* Reduces **variance** and helps prevent overfitting.\n",
        "* Allows **Out-of-Bag (OOB) estimation** by leaving out ~1/3 of data in each sample, providing an unbiased performance estimate.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* Random Forest uses bootstrap sampling to train each decision tree on a different subset of data.\n",
        "\n",
        "**Conclusion:**\n",
        "Bootstrap sampling in Bagging is essential for generating diverse training sets, which enables the ensemble to **reduce variance, avoid overfitting, and produce more accurate and stable predictions**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUw9KFamVk-D"
      },
      "source": [
        "**19. What are some real-world applications of ensemble techniques?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Real-World Applications of Ensemble Techniques**\n",
        "\n",
        "Ensemble techniques are widely used in machine learning because they **improve accuracy, robustness, and generalization**. They are applied in various domains where high predictive performance is critical.\n",
        "\n",
        "**Key Applications:**\n",
        "\n",
        "1. **Finance and Banking:**\n",
        "\n",
        "   * **Credit scoring** to assess loan eligibility.\n",
        "   * **Fraud detection** to identify suspicious transactions.\n",
        "   * **Stock price prediction** using regression ensembles.\n",
        "\n",
        "2. **Healthcare and Medical Diagnosis:**\n",
        "\n",
        "   * Disease prediction and diagnosis from patient data.\n",
        "   * Medical image analysis (e.g., detecting tumors in MRI or X-ray scans).\n",
        "\n",
        "3. **E-commerce and Recommendation Systems:**\n",
        "\n",
        "   * Product recommendation using ensemble models to combine different algorithms.\n",
        "   * Customer segmentation and churn prediction.\n",
        "\n",
        "4. **Natural Language Processing (NLP):**\n",
        "\n",
        "   * Sentiment analysis using multiple classifiers.\n",
        "   * Spam detection in emails.\n",
        "\n",
        "5. **Computer Vision:**\n",
        "\n",
        "   * Object detection and image classification (e.g., combining CNN models).\n",
        "   * Facial recognition systems.\n",
        "\n",
        "6. **Weather and Climate Forecasting:**\n",
        "\n",
        "   * Predicting rainfall, temperature, or extreme weather events by combining multiple models.\n",
        "\n",
        "7. **Cybersecurity:**\n",
        "\n",
        "   * Intrusion detection systems using ensemble classifiers.\n",
        "   * Malware detection in software systems.\n",
        "\n",
        "**Conclusion:**\n",
        "Ensemble techniques are applied across **finance, healthcare, e-commerce, NLP, computer vision, and cybersecurity** to improve predictive accuracy, robustness, and reliability, making them highly effective for real-world machine learning problems.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcsDA9HXVm2t"
      },
      "source": [
        "**20. What is the difference between Bagging and Boosting?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Difference Between Bagging and Boosting**\n",
        "\n",
        "Bagging and Boosting are both ensemble techniques, but they differ in **how the base models are trained and combined**.\n",
        "\n",
        "**Bagging:**\n",
        "\n",
        "* Base models are trained **independently** on different **bootstrap samples** of the training data.\n",
        "* Predictions are combined by **majority voting** (for classification) or **averaging** (for regression).\n",
        "* Primarily reduces **variance** and helps prevent overfitting.\n",
        "* Less sensitive to noisy data.\n",
        "* Example: Random Forest.\n",
        "\n",
        "**Boosting:**\n",
        "\n",
        "* Base models are trained **sequentially**, with each new model focusing on the **errors of the previous models**.\n",
        "* Predictions are combined using **weighted voting or weighted sum**, giving more importance to accurate models.\n",
        "* Primarily reduces **bias** and improves accuracy.\n",
        "* More sensitive to noisy data because it emphasizes hard-to-predict points.\n",
        "* Examples: AdaBoost, Gradient Boosting, XGBoost.\n",
        "\n",
        "**Conclusion:**\n",
        "Bagging builds multiple independent models to **reduce variance**, whereas Boosting builds sequential models that **focus on correcting errors to reduce bias**. Both methods improve prediction accuracy but in different ways.\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkXMSGKxV0UG"
      },
      "source": [
        "##Practical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY1-LUWmV6-L"
      },
      "source": [
        "21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVygU7jwWXEo",
        "outputId": "04cc4149-c19d-48c8-d0bd-0f24fdf919cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3uXFRwdWa1b"
      },
      "source": [
        "22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_UkRklYWYjG",
        "outputId": "00f1c997-86b8-4356-e04c-95ebb799ae54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.25787382250585034\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JavBsPIUXD2m"
      },
      "source": [
        "23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIy61AgoWzUC",
        "outputId": "0653c844-0f20-4ed0-b7c3-6c04cf2d9db8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean radius: 0.0323\n",
            "mean texture: 0.0111\n",
            "mean perimeter: 0.0601\n",
            "mean area: 0.0538\n",
            "mean smoothness: 0.0062\n",
            "mean compactness: 0.0092\n",
            "mean concavity: 0.0806\n",
            "mean concave points: 0.1419\n",
            "mean symmetry: 0.0033\n",
            "mean fractal dimension: 0.0031\n",
            "radius error: 0.0164\n",
            "texture error: 0.0032\n",
            "perimeter error: 0.0118\n",
            "area error: 0.0295\n",
            "smoothness error: 0.0059\n",
            "compactness error: 0.0046\n",
            "concavity error: 0.0058\n",
            "concave points error: 0.0034\n",
            "symmetry error: 0.0040\n",
            "fractal dimension error: 0.0071\n",
            "worst radius: 0.0780\n",
            "worst texture: 0.0188\n",
            "worst perimeter: 0.0743\n",
            "worst area: 0.1182\n",
            "worst smoothness: 0.0118\n",
            "worst compactness: 0.0175\n",
            "worst concavity: 0.0411\n",
            "worst concave points: 0.1271\n",
            "worst symmetry: 0.0129\n",
            "worst fractal dimension: 0.0069\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "importance_scores = model.feature_importances_\n",
        "\n",
        "for name, score in zip(feature_names, importance_scores):\n",
        "    print(f\"{name}: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE2V7vuiXWwb"
      },
      "source": [
        "24. Train a Random Forest Regressor and compare its performance with a single Decision Tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42wY6bx-XL2p",
        "outputId": "e393b0ce-220c-41d8-a40d-045bbd17b42c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree MSE: 0.5280096503174904\n",
            "Random Forest MSE: 0.25650512920799395\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree Regressor\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Decision Tree MSE:\", mse_dt)\n",
        "print(\"Random Forest MSE:\", mse_rf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0SdE--0X1oP"
      },
      "source": [
        "25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NlAUbXrXkbv",
        "outputId": "65ebd506-f7bd-47ff-aed3-d317c780096b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out-of-Bag (OOB) Score: 0.9547738693467337\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Out-of-Bag (OOB) Score:\", model.oob_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ITQq5GUYBhv"
      },
      "source": [
        "26. Train a Bagging Classifier using SVM as a base estimator and print accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAUFkfkSX9fP",
        "outputId": "ae6b581d-c0cc-4596-e08d-afffe9fe88c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = BaggingClassifier(estimator=SVC(), n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1OQdcVqYG4S"
      },
      "source": [
        "27. Train a Random Forest Classifier with different numbers of trees and compare accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiPF5gyxYFTj",
        "outputId": "a2a37fd8-34e5-4a7e-8d90-35bde039c9a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Trees: 10, Accuracy: 0.9649\n",
            "Number of Trees: 50, Accuracy: 0.9708\n",
            "Number of Trees: 100, Accuracy: 0.9708\n",
            "Number of Trees: 200, Accuracy: 0.9708\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "n_trees_list = [10, 50, 100, 200]\n",
        "for n_trees in n_trees_list:\n",
        "    model = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Number of Trees: {n_trees}, Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQHL8eBvYWWQ"
      },
      "source": [
        "28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dH7_JYIYRxb",
        "outputId": "b912782a-5df0-4893-c033-b2d23730bf3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score: 0.998236331569665\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = BaggingClassifier(estimator=LogisticRegression(max_iter=2000), n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"AUC Score:\", auc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isLJvl0rYiQH"
      },
      "source": [
        "29. Train a Random Forest Regressor and analyze feature importance scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T-uqx4mYdIQ",
        "outputId": "90442660-2c56-408d-a036-0bcdcd8aa5d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MedInc: 0.5260\n",
            "HouseAge: 0.0547\n",
            "AveRooms: 0.0472\n",
            "AveBedrms: 0.0300\n",
            "Population: 0.0317\n",
            "AveOccup: 0.1382\n",
            "Latitude: 0.0861\n",
            "Longitude: 0.0861\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "importance_scores = model.feature_importances_\n",
        "\n",
        "for name, score in zip(feature_names, importance_scores):\n",
        "    print(f\"{name}: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-HwehvAZQ5G"
      },
      "source": [
        "30. Train an ensemble model using both Bagging and Random Forest and compare accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlC--EmpZK9D",
        "outputId": "59cca370-6551-4264-de18-b45eb748a8ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.9590643274853801\n",
            "Random Forest Accuracy: 0.9707602339181286\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Classifier with Decision Tree\n",
        "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_model.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy_bagging)\n",
        "print(\"Random Forest Accuracy:\", accuracy_rf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Paflp3FOZeun"
      },
      "source": [
        "31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vAKicxqZV9R",
        "outputId": "dbc962ac-4672-4263-f46b-76e53cfe9b50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'n_estimators': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None}\n",
            "Accuracy: 0.9707602339181286\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist,\n",
        "                                   n_iter=20, cv=3, n_jobs=-1, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ootBq2hFZqEz"
      },
      "source": [
        "32. Train a Bagging Regressor with different numbers of base estimators and compare performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK0HET6RZm3T",
        "outputId": "e961254e-7097-4df7-90e2-04225bb1e4b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Estimators: 10, MSE: 0.2862\n",
            "Number of Estimators: 50, MSE: 0.2579\n",
            "Number of Estimators: 100, MSE: 0.2568\n",
            "Number of Estimators: 200, MSE: 0.2542\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "\n",
        "for n in n_estimators_list:\n",
        "    model = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=n, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Number of Estimators: {n}, MSE: {mse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Train a Random Forest Classifier and analyze misclassified samples."
      ],
      "metadata": {
        "id": "IJ-v_VC4hNyT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6FPJ9EY5ZzDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2846563-88ae-43ed-cc6e-b93b931dafe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Misclassified Samples: 5\n",
            "Indices of Misclassified Samples: [8, 20, 77, 82, 164]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "misclassified_indices = [i for i, (true, pred) in enumerate(zip(y_test, y_pred)) if true != pred]\n",
        "\n",
        "print(\"Number of Misclassified Samples:\", len(misclassified_indices))\n",
        "print(\"Indices of Misclassified Samples:\", misclassified_indices)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier."
      ],
      "metadata": {
        "id": "6suaCe1_henq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Bagging Classifier with Decision Tree\n",
        "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_model.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", accuracy_dt)\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy_bagging)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7nzKuuDhYhd",
        "outputId": "a82c6475-984e-4a41-e23f-ee0f085cab53"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Train a Random Forest Classifier and visualize the confusion matrix."
      ],
      "metadata": {
        "id": "DIw2Cduqhqbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
        "disp.plot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "842HV2_jhleg",
        "outputId": "795eded4-0c97-4edd-a23e-a4b779e0757e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7a80937aef60>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy."
      ],
      "metadata": {
        "id": "y6jd3vqah0Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('svm', SVC(probability=True)),\n",
        "    ('lr', LogisticRegression(max_iter=1000))\n",
        "]\n",
        "\n",
        "stacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = stacking_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVtP_oFwhvPp",
        "outputId": "591739f1-5269-4a70-d911-afeea785f54a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a Random Forest Classifier and print the top 5 most important features."
      ],
      "metadata": {
        "id": "wr5AWdpVh6WM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1][:5]\n",
        "\n",
        "for i in indices:\n",
        "    print(f\"{feature_names[i]}: {importances[i]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G0JaIiRh5Wi",
        "outputId": "a2190b05-813c-442f-c0a4-abb559190254"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean concave points: 0.1419\n",
            "worst concave points: 0.1271\n",
            "worst area: 0.1182\n",
            "mean concavity: 0.0806\n",
            "worst radius: 0.0780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score."
      ],
      "metadata": {
        "id": "bJfmCakKiGej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9i95i9tiDC2",
        "outputId": "b0c1d9d9-33c0-4140-f142-a96946c3da7f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy."
      ],
      "metadata": {
        "id": "XOx-ZWlMiR3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "max_depth_values = [None, 2, 4, 6, 8, 10]\n",
        "\n",
        "for depth in max_depth_values:\n",
        "    model = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Max Depth: {depth}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeenjXJdiOM-",
        "outputId": "552a724a-abdb-4295-c684-43e83a271eef"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Depth: None, Accuracy: 0.9708\n",
            "Max Depth: 2, Accuracy: 0.9532\n",
            "Max Depth: 4, Accuracy: 0.9708\n",
            "Max Depth: 6, Accuracy: 0.9649\n",
            "Max Depth: 8, Accuracy: 0.9708\n",
            "Max Depth: 10, Accuracy: 0.9708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance."
      ],
      "metadata": {
        "id": "Y2g_RXwSifOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging with Decision Tree\n",
        "dt_model = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "# Bagging with K-Neighbors\n",
        "knn_model = BaggingRegressor(estimator=KNeighborsRegressor(), n_estimators=50, random_state=42)\n",
        "knn_model.fit(X_train, y_train)\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "\n",
        "print(\"Bagging with Decision Tree MSE:\", mse_dt)\n",
        "print(\"Bagging with K-Neighbors MSE:\", mse_knn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iQnGNjKiWlV",
        "outputId": "609d6c35-344f-46ca-b310-8803dd0de426"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging with Decision Tree MSE: 0.25787382250585034\n",
            "Bagging with K-Neighbors MSE: 1.1020902555745289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score."
      ],
      "metadata": {
        "id": "YLRlihMUip4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7oAJAMJimr8",
        "outputId": "063f725e-6750-44e8-caba-da4dc19f66de"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9968400940623163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a Bagging Classifier and evaluate its performance using cross-validatio."
      ],
      "metadata": {
        "id": "zoLlZOYhi0GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "print(\"Cross-Validation Accuracy Scores:\", scores)\n",
        "print(\"Mean Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j4MzR4fiv5v",
        "outputId": "a6af8521-4670-4813-c0a3-7c4b99c17838"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Accuracy Scores: [0.96666667 0.96666667 0.93333333 0.96666667 1.        ]\n",
            "Mean Accuracy: 0.9666666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Train a Random Forest Classifier and plot the Precision-Recall curv."
      ],
      "metadata": {
        "id": "IBPO3f-yi8ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "Bd7XQ3tUi54Y",
        "outputId": "0e8fb282-b9fa-46ca-d461-92ef70478640"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7a80b6997110>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy."
      ],
      "metadata": {
        "id": "8TZ4nM4MjJZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "]\n",
        "\n",
        "stacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=1000))\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = stacking_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6lAnBLIjCab",
        "outputId": "278fc740-874c-43a1-a2e3-a0e79e13c79c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance."
      ],
      "metadata": {
        "id": "G8NjWdHTjQi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "bootstrap_options = [True, False]\n",
        "\n",
        "for bootstrap in bootstrap_options:\n",
        "    model = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, bootstrap=bootstrap, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Bootstrap={bootstrap}, MSE={mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNC1lJaqjND5",
        "outputId": "106bb6e9-adb9-4aa8-ff87-a0e3c5d58eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrap=True, MSE=0.2579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "KbYrLXsAkQep"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}